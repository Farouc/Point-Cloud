{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzkI9Pa6yZus",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5aeca552-edc5-4fa1-ca0e-7216853a3376"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting trimesh\n",
            "  Downloading trimesh-4.3.2-py3-none-any.whl (693 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/693.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/693.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m686.1/693.9 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m693.9/693.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from trimesh) (1.25.2)\n",
            "Installing collected packages: trimesh\n",
            "Successfully installed trimesh-4.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install trimesh\n",
        "import os\n",
        "import glob\n",
        "import trimesh\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "tf.random.set_seed(1234)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use hardware accelerator for training\n",
        "#physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "#print(\"GPUs Available: \", len(physical_devices))\n",
        "#tf.config.experimental.set_memory_growth(physical_devices[0], True)"
      ],
      "metadata": {
        "id": "q8-NON1a330i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the Dataset"
      ],
      "metadata": {
        "id": "vpoKVdmI4R1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = tf.keras.utils.get_file(\n",
        "    \"modelnet.zip\",\n",
        "    \"http://3dvision.princeton.edu/projects/2014/3DShapeNets/ModelNet10.zip\",\n",
        "    extract=True,\n",
        ")\n",
        "DATA_DIR = os.path.join(os.path.dirname(DATA_DIR), \"ModelNet10\")"
      ],
      "metadata": {
        "id": "fHYpm58R4UtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize a Mesh from the Dataset\n"
      ],
      "metadata": {
        "id": "kyJymsjF4Z0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mesh = trimesh.load(os.path.join(DATA_DIR, \"bed/train/bed_0001.off\"))\n",
        "mesh.show()"
      ],
      "metadata": {
        "id": "YkOBLevs4aiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample a Mesh and Show Result"
      ],
      "metadata": {
        "id": "66frY_Sc4fnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rtree"
      ],
      "metadata": {
        "id": "VVYey2DD4ggo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "points_test=trimesh.sample.sample_surface(mesh, count=2048)\n",
        "faces=points_test[1]\n",
        "print(faces)\n",
        "points_test=points_test[0]\n",
        "\n",
        "fig = plt.figure(figsize=(5, 5))\n",
        "ax = fig.add_subplot(111, projection=\"3d\")\n",
        "ax.scatter(points_test[:, 0], points_test[:, 1], points_test[:, 2])\n",
        "ax.set_axis_off()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lvIjNPJu4kQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Function to Parse Data\n"
      ],
      "metadata": {
        "id": "Zuq499Bn8V6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_dataset(num_points=2048):\n",
        "\n",
        "    train_points = []\n",
        "    train_labels = []\n",
        "    test_points = []\n",
        "    test_labels = []\n",
        "    class_map = {}\n",
        "    folders = glob.glob(os.path.join(DATA_DIR, \"[!README]*\"))\n",
        "\n",
        "    for i, folder in enumerate(folders):\n",
        "        print(\"processing class: {}\".format(os.path.basename(folder)))\n",
        "        # store folder name with ID so we can retrieve later\n",
        "        class_map[i] = folder.split(\"/\")[-1]\n",
        "        # gather all files\n",
        "        train_files = glob.glob(os.path.join(folder, \"train/*\"))\n",
        "        test_files = glob.glob(os.path.join(folder, \"test/*\"))\n",
        "\n",
        "        for f in train_files:\n",
        "            train_points.append(trimesh.load(f).sample(num_points))\n",
        "            train_labels.append(i)\n",
        "\n",
        "        for f in test_files:\n",
        "            test_points.append(trimesh.load(f).sample(num_points))\n",
        "            test_labels.append(i)\n",
        "\n",
        "    return (\n",
        "        np.array(train_points),\n",
        "        np.array(test_points),\n",
        "        np.array(train_labels),\n",
        "        np.array(test_labels),\n",
        "        class_map,\n",
        "    )"
      ],
      "metadata": {
        "id": "vcug1dRx8c8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Parse the Dataset and Process each Class\n"
      ],
      "metadata": {
        "id": "tjiVay4z81sZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_POINTS = 2048\n",
        "NUM_CLASSES = 10\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_points, test_points, train_labels, test_labels, CLASS_MAP = parse_dataset(NUM_POINTS)"
      ],
      "metadata": {
        "id": "SGgDNtLy8jDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Augenmentation for Train Dataset\n"
      ],
      "metadata": {
        "id": "ECtqrU-H9CuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def augment(points, label):\n",
        "    # jitter points\n",
        "    points += tf.random.uniform(points.shape, -0.005, 0.005, dtype=tf.float64)\n",
        "    # shuffle points\n",
        "    points = tf.random.shuffle(points)\n",
        "    return points, label\n",
        "\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_points, train_labels))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_points, test_labels))\n",
        "\n",
        "train_dataset = train_dataset.shuffle(len(train_points)).map(augment).batch(BATCH_SIZE)\n",
        "test_dataset = test_dataset.shuffle(len(test_points)).batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "2Ideltsl8-3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset_normal = tf.data.Dataset.from_tensor_slices((test_points_normal, test_labels_normal))\n",
        "test_dataset_normal = test_dataset_normal.shuffle(len(test_points_normal)).batch(BATCH_SIZE)\n"
      ],
      "metadata": {
        "id": "pk5-OCVo9KXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Functions to Build the Model\n"
      ],
      "metadata": {
        "id": "nUlxbGuG9Mkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_bn(x, filters):\n",
        "    x = layers.Conv1D(filters, kernel_size=1, padding=\"valid\")(x)\n",
        "    x = layers.BatchNormalization(momentum=0.0)(x)\n",
        "    return layers.Activation(\"relu\")(x)\n",
        "\n",
        "\n",
        "def dense_bn(x, filters):\n",
        "    x = layers.Dense(filters)(x)\n",
        "    x = layers.BatchNormalization(momentum=0.0)(x)\n",
        "    return layers.Activation(\"relu\")(x)"
      ],
      "metadata": {
        "id": "Yp_kSml99O4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OrthogonalRegularizer(keras.regularizers.Regularizer):\n",
        "    def __init__(self, num_features, l2reg=0.001):\n",
        "        self.num_features = num_features\n",
        "        self.l2reg = l2reg\n",
        "        self.eye = tf.eye(num_features)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = tf.reshape(x, (-1, self.num_features, self.num_features))\n",
        "        xxt = tf.tensordot(x, x, axes=(2, 2))\n",
        "        xxt = tf.reshape(xxt, (-1, self.num_features, self.num_features))\n",
        "        return tf.reduce_sum(self.l2reg * tf.square(xxt - self.eye))"
      ],
      "metadata": {
        "id": "o888Rtn39R4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Function to create T-net Layers\n"
      ],
      "metadata": {
        "id": "aY3nV1aj9YX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tnet(inputs, num_features):\n",
        "\n",
        "    # Initalise bias as the indentity matrix\n",
        "    bias = keras.initializers.Constant(np.eye(num_features).flatten())\n",
        "    reg = OrthogonalRegularizer(num_features)\n",
        "\n",
        "    x = conv_bn(inputs, 32)\n",
        "    x = conv_bn(x, 64)\n",
        "    x = conv_bn(x, 512)\n",
        "    x = layers.GlobalMaxPooling1D()(x)\n",
        "    x = dense_bn(x, 256)\n",
        "    x = dense_bn(x, 128)\n",
        "    x = layers.Dense(\n",
        "        num_features * num_features,\n",
        "        kernel_initializer=\"zeros\",\n",
        "        bias_initializer=bias,\n",
        "        activity_regularizer=reg,\n",
        "    )(x)\n",
        "    feat_T = layers.Reshape((num_features, num_features))(x)\n",
        "    # Apply affine transformation to input features\n",
        "    return layers.Dot(axes=(2, 1))([inputs, feat_T])"
      ],
      "metadata": {
        "id": "0JlJT9II9VIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create the Convolutional Neural Network\n"
      ],
      "metadata": {
        "id": "qkiMVRdM9gON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(NUM_POINTS, 3))\n",
        "\n",
        "x = tnet(inputs, 3)\n",
        "x = conv_bn(x, 64)\n",
        "x = conv_bn(x, 64)\n",
        "x = tnet(x, 64)\n",
        "x = conv_bn(x, 64)\n",
        "x = conv_bn(x, 128)\n",
        "x = conv_bn(x, 1024)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = dense_bn(x, 512)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "x = dense_bn(x, 256)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "\n",
        "outputs = layers.Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs, name=\"pointnet\")\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "qMHWuJS09eTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Compile and Train the Model\n"
      ],
      "metadata": {
        "id": "GLFtI5Ym9qn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(\"/content/pointnet_poids.h5\")\n"
      ],
      "metadata": {
        "id": "HMisMLsT9nl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    metrics=[\"sparse_categorical_accuracy\"],\n",
        ")\n",
        "\n",
        "model.fit(train_dataset, epochs=50, validation_data=test_dataset)"
      ],
      "metadata": {
        "id": "0OyxOaaQ9vFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "points = test_dataset\n",
        "preds = model.predict(points)\n",
        "preds = tf.math.argmax(preds, -1)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = np.mean((preds == labels).numpy())\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "d2OIezhQ-DpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  l'ajout de rotation au data"
      ],
      "metadata": {
        "id": "5pA9185SSRBr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aP-uBXm7LNam",
        "outputId": "c6cb3d85-2fcb-4a3b-c533-96f513619806"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3991\n"
          ]
        }
      ],
      "source": [
        "print (len(train_points))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Avec distribution fixe"
      ],
      "metadata": {
        "id": "-rPBHJBVsDwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rotate_point_cloud_z(point_cloud, angle_degrees):\n",
        "    angle_radians = np.radians(angle_degrees)\n",
        "    rotation_matrix = np.array([\n",
        "        [np.cos(angle_radians), -np.sin(angle_radians), 0],\n",
        "        [np.sin(angle_radians), np.cos(angle_radians), 0],\n",
        "        [0, 0, 1]\n",
        "    ])\n",
        "    rotated_point_cloud = np.dot(point_cloud, rotation_matrix.T)\n",
        "    return rotated_point_cloud"
      ],
      "metadata": {
        "id": "a0EpNdZGsDTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_class_data_with_rotation(class_data, num_duplicates=1):\n",
        "    augmented_data = []\n",
        "    angles = [40, 80, 120, 170]  # Liste des angles de rotation\n",
        "    for angle in angles:\n",
        "        for _ in range(num_duplicates):\n",
        "            # Rotation de chaque élément de classe\n",
        "            rotated_data = rotate_point_cloud_z(class_data, angle)\n",
        "            augmented_data.append(rotated_data)\n",
        "    augmented_data.append(class_data)\n",
        "    return np.vstack(augmented_data)"
      ],
      "metadata": {
        "id": "lYE3Ty0MsQiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_dataset_with_rotation(data, labels, num_duplicates=4):\n",
        "    augmented_data = []\n",
        "    augmented_labels = []\n",
        "    unique_labels = np.unique(labels)\n",
        "    for label in unique_labels:\n",
        "        class_indices = np.where(labels == label)[0]\n",
        "        class_data = data[class_indices]\n",
        "        for _ in range(num_duplicates):\n",
        "            # Ajout de rotation à chaque élément de classe avec bruit\n",
        "            augmented_class_data = augment_class_data_with_rotation(class_data)\n",
        "            max_shape = max([data.shape for data in augmented_class_data])\n",
        "            augmented_class_data_resized = [np.pad(data, ((0, max_shape[0] - data.shape[0]), (0, 0)), mode='constant') for data in augmented_class_data]\n",
        "            augmented_data.extend(augmented_class_data_resized)\n",
        "            augmented_labels.extend([label] * len(augmented_class_data_resized))\n",
        "    return np.array(augmented_data), np.array(augmented_labels)"
      ],
      "metadata": {
        "id": "7ymxeyJvsUcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utilisation de la fonction pour augmenter les données avec bruit et rotation\n",
        "train_points_augmented_with_rotation, train_labels_augmented_with_rotation = augment_dataset_with_rotation(train_points, train_labels, num_duplicates=4)\n",
        "test_points_augmented_with_rotation, test_labels_augmented_with_rotation = augment_dataset_with_rotation(test_points, test_labels, num_duplicates=4)\n"
      ],
      "metadata": {
        "id": "y35iGLfcsWj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    metrics=[\"sparse_categorical_accuracy\"],\n",
        ")\n",
        "\n",
        "model.fit(train_points_augmented_with_rotation, epochs=30, validation_data=test_dataset)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IkvTJFyDqaEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcul de l'accuracy sur les données de test augmentées\n",
        "points = test_points_augmented_with_rotation\n",
        "labels = test_labels_augmented_with_rotation\n",
        "\n",
        "# Run test data through model\n",
        "preds = model.predict(points)\n",
        "preds = tf.math.argmax(preds, -1)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = np.mean((preds == labels).numpy())\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "hDO7-HJMqaEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Avec Distribution aleatoire"
      ],
      "metadata": {
        "id": "StrJ49VWsm66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rotate_point_cloud_z(point_cloud,angle_degrees):\n",
        "  angle_degrees = np.random(10,360)\n",
        "  rotation_matrix=np.array([\n",
        "      [np.cos(angle_degrees),-np.sin(angle_degrees),0],\n",
        "      [np.sin(angle_degrees),np.cos(angle_degrees),0],\n",
        "      [0,0,1]\n",
        "  ])"
      ],
      "metadata": {
        "id": "bTaCjIBdsnHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_class_data_with_rotation(class_data, num_duplicates=4):\n",
        "    augmented_data = []\n",
        "    for _ in range(num_duplicates):\n",
        "\n",
        "        rotated_data = rotate_point_cloud_z(class_data, angle)\n",
        "        augmented_data.append(rotated_data)\n",
        "    augmented_data.append(class_data)\n",
        "    return np.vstack(augmented_data)"
      ],
      "metadata": {
        "id": "hvldi_JhtMw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_dataset_with_rotation(data, labels, num_duplicates=4):\n",
        "    augmented_data = []\n",
        "    augmented_labels = []\n",
        "    unique_labels = np.unique(labels)\n",
        "    for label in unique_labels:\n",
        "        class_indices = np.where(labels == label)[0]\n",
        "        class_data = data[class_indices]\n",
        "        for _ in range(num_duplicates):\n",
        "            # Ajout de rotation à chaque élément de classe avec bruit\n",
        "            augmented_class_data = augment_class_data_with_rotation(class_data)\n",
        "            max_shape = max([data.shape for data in augmented_class_data])\n",
        "            augmented_class_data_resized = [np.pad(data, ((0, max_shape[0] - data.shape[0]), (0, 0)), mode='constant') for data in augmented_class_data]\n",
        "            augmented_data.extend(augmented_class_data_resized)\n",
        "            augmented_labels.extend([label] * len(augmented_class_data_resized))\n",
        "    return np.array(augmented_data), np.array(augmented_labels)"
      ],
      "metadata": {
        "id": "3-VnICPxtNFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utilisation de la fonction pour augmenter les données avec bruit et rotation\n",
        "train_points_augmented_with_rotation, train_labels_augmented_with_rotation = augment_dataset_with_rotation(train_points, train_labels, num_duplicates=4)\n",
        "test_points_augmented_with_rotation, test_labels_augmented_with_rotation = augment_dataset_with_rotation(test_points, test_labels, num_duplicates=4)\n"
      ],
      "metadata": {
        "id": "1S0wvnb8teHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    metrics=[\"sparse_categorical_accuracy\"],\n",
        ")\n",
        "\n",
        "model.fit(train_points_augmented_with_rotation, epochs=30, validation_data=test_dataset)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XODh8XoVqbtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcul de l'accuracy sur les données de test augmentées\n",
        "points = test_points_augmented_with_rotation\n",
        "labels = test_labels_augmented_with_rotation\n",
        "\n",
        "# Run test data through model\n",
        "preds = model.predict(points)\n",
        "preds = tf.math.argmax(preds, -1)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = np.mean((preds == labels).numpy())\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "P7NpzUaeqbts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Avec Distribution Uniforme"
      ],
      "metadata": {
        "id": "Tb5eehZ0RfFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rotatio_z(point_cloud,a):\n",
        "  a = np.random.uniform(10,360)\n",
        "  rotation_matrix=np.array([\n",
        "      [np.cos(a),-np.sin(a),0],\n",
        "      [np.sin(a),np.cos(a),0],\n",
        "      [0,0,1]\n",
        "  ])"
      ],
      "metadata": {
        "id": "cvd0525MIYcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_class_data_with_rotation(class_data, num_duplicates=4):\n",
        "\n",
        "  augmented_data = []\n",
        "  for _ in range(num_duplicates):\n",
        "    a=np.random.uniform(10,360)\n",
        "    rotated_data = rotatio_z(class_data, a)\n",
        "    augmented_data.append(rotated_data)\n",
        "  augmented_data.append(class_data)\n",
        "  return np.vstack(augmented_data)"
      ],
      "metadata": {
        "id": "4hzbJIFmORbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_dataset_with_rotation(data, labels, num_duplicates=4):\n",
        "    augmented_data = []\n",
        "    augmented_labels = []\n",
        "    unique_labels = np.unique(labels)\n",
        "    max_shape = max([data.shape for data in data])\n",
        "    for label in unique_labels:\n",
        "        class_indices = np.where(labels == label)[0]\n",
        "        class_data = data[class_indices]\n",
        "        for _ in range(num_duplicates):\n",
        "            # Ajout de rotation à chaque élément de classe avec bruit\n",
        "            augmented_class_data = augment_class_data_with_rotation(class_data)\n",
        "            augmented_class_data_resized = [np.pad(data, ((0, max_shape[0] - data.shape[0]), (0, 0)), mode='constant') for data in augmented_class_data]\n",
        "            augmented_data.extend(augmented_class_data_resized)\n",
        "            augmented_labels.extend([label] * len(augmented_class_data_resized))\n",
        "    return np.array(augmented_data), np.array(augmented_labels)\n",
        "\n",
        "# Utilisation de la fonction pour augmenter les données avec bruit et rotation\n",
        "train_points_augmented_with_rotation, train_labels_augmented_with_rotation = augment_dataset_with_rotation(train_points, train_labels, num_duplicates=4)\n",
        "test_points_augmented_with_rotation, test_labels_augmented_with_rotation = augment_dataset_with_rotation(test_points, test_labels, num_duplicates=4)"
      ],
      "metadata": {
        "id": "fp2UOkOuNc8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    metrics=[\"sparse_categorical_accuracy\"],\n",
        ")\n",
        "\n",
        "model.fit(train_points_augmented_with_rotation, epochs=30, validation_data=test_dataset)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ggo1VLdJSmw8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "bdf7120e-c1df-4e12-8787-b1a666db3672"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-c187f467b713>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.compile(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sparse_categorical_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sparse_categorical_accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcul de l'accuracy sur les données de test augmentées\n",
        "points = test_points_augmented_with_rotation\n",
        "labels = test_labels_augmented_with_rotation\n",
        "\n",
        "# Run test data through model\n",
        "preds = model.predict(points)\n",
        "preds = tf.math.argmax(preds, -1)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = np.mean((preds == labels).numpy())\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "EK1Jns1Jog2C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}